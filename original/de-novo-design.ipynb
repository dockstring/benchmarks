{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The dockstring _de novo_ molecular design (aka molecular optimization) benchmark\n",
    "\n",
    "### Summary\n",
    "\n",
    "In this tutorial we evaluate a dummy algorithm on a _mini version_ of dockstring's de novo molecular design benchmarks.\n",
    "These methods are very simple, and the performance is expected to be very poor.\n",
    "The actual dockstring benchmark has a budget of 5000 calls to the objective function,\n",
    "which takes a very long time to run (at least 1 day).\n",
    "For the purposes of this tutorial, we limit the budget to just 5 objective function evaluations,\n",
    "but if you are using this notebook to test your method then you should replace this.\n",
    "\n",
    "### Method tested\n",
    "\n",
    "The method is a model-free method which uses the dataset of pre-computed values, and treats the objective functions as _white box_ (i.e. one is allowed to directly use the docking and QED scores used to compute the objective function value). It randomly chooses one of the top 10,000 known SMILES, modifies it by adding a single carbon atom to the beginning of the string, and returns it if the resulting SMILES is valid _and_ it if its QED value does not change too much. It can be thought of as a very simple genetic algorithm.\n",
    "\n",
    "### FAQ\n",
    "\n",
    "If you are replacing this code with your own algorithm, here are the answers to some potential questions that you may have:\n",
    "\n",
    "- _Do I need to use the dataset_? No, your method can ignore the dataset if if it not needed.\n",
    "- _Is there a limit on how often the QED function can be called?_: No, this function is very fast. The limit only applies to the docking functions\n",
    "- _Do I need to use the benchmark API provided by dockstring?_ It is highly recommended but not strictly necessary.\n",
    "- _Do I need to use LRU cache?_ No, but this option is \"safer\" than counting function calls manually because it makes it very difficult to count incorrectly.\n",
    "- _For objectives involving multiple calls to dockstring (e.g. selective JAK2 which docks JAK2 and LCK), can we only call dockstring on a subset of the objectives?_ For comparing to other methods on the original benchmark, our suggestion is _no_, but feel free to experiment with this problem setting yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECTIVE_EVAL_BUDGET = 5\n",
    "# OBJECTIVE_EVAL_BUDGET = 5000  # *uncomment this line to run the true benchmark*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import everything- install missing libraries as needed\n",
    "# (everything here is quite standard)\n",
    "import random\n",
    "import functools\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from tqdm.auto import tqdm  # Helpful progress bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Import dockstring benchmark functions\n",
    "\n",
    "The dockstring package has an API for accessing the benchmark objective functions.\n",
    "We demonstrate its use below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dockstring.benchmarks.original import get_benchmark_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the benchmark functions, optionally specifying the number of CPUs\n",
    "# Result is a dictionary containing 3 callable objects\n",
    "benchmark_function_dict = get_benchmark_functions(num_cpus=8)  # TODO: adjust if needed\n",
    "print(f\"Keys: {benchmark_function_dict.keys()}\")\n",
    "benchmark_function_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main method of interacting with these functions is calling them with a SMILES string\n",
    "# They return a tuple (v, d) where v is the objective value to be minimized,\n",
    "# and d is a dictionary of intermediate values\n",
    "sample_benchmark_fn = benchmark_function_dict[\"selective_JAK2\"]\n",
    "sample_smiles = \"CCCC\"\n",
    "sample_benchmark_fn(sample_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If desired, the objective can be computed from the intermeidate values as so:\n",
    "sample_benchmark_fn.aggregation_function(QED=0.4310, JAK2=-2.7, LCK=-2.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of required intermediate values can be viewed as so:\n",
    "sample_benchmark_fn.base_functions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If desired, the functions can be called individually as so:\n",
    "sample_benchmark_fn.base_functions[\"JAK2\"](sample_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup, so the rest of the notebook is not affected\n",
    "del sample_benchmark_fn, sample_smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Downloading the dockstring dataset\n",
    "\n",
    "This is only necessary if your algorithm will use the pre-computed values in the dockstring dataset.\n",
    "If you haven't downloaded the dataset,\n",
    "follow the instructions [here](https://github.com/dockstring/dataset)\n",
    "to download the data into a folder called `data` in the root of this repository.\n",
    "If this is done correctly, the following cell should run without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the necessary data files are present, this cell will run without error\n",
    "dataset_path = Path(\"../data/dockstring-dataset.tsv\")\n",
    "assert dataset_path.exists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading the dataset and filling in known objective function values\n",
    "\n",
    "We read in the `.tsv` file, fill in the QED values for all molecules, and compute the objective values using the API above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset using pandas (using \"\\t\" since it is tab-delimited)\n",
    "df = pd.read_csv(dataset_path, sep=\"\\t\").set_index(\"inchikey\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the QED for the dataset\n",
    "df[\"QED\"] = [Chem.QED.qed(Chem.MolFromSmiles(str(s))) for s in tqdm(df.smiles)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert values for all objective functions into the dataframe\n",
    "for benchmark_name, benchmark_func in benchmark_function_dict.items():\n",
    "    print(benchmark_name)\n",
    "    \n",
    "    # Get all the required values from the dataframe\n",
    "    required_values = {prop_name: df[prop_name].values for prop_name in benchmark_func.base_functions.keys()}\n",
    "    \n",
    "    # Calculate objective function values\n",
    "    objective_values = [benchmark_func.aggregation_function(**{k: v[i] for k, v in required_values.items()}) for i in range(len(df))]\n",
    "    \n",
    "    # Append these to the dataframe, using the prefix \"obj\" to avoid name collisions\n",
    "    df[f\"obj_{benchmark_name}\"] = objective_values\n",
    "    \n",
    "    del benchmark_name, benchmark_func, objective_values, required_values\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that the pre-computed values match the real objective functions for an arbitrary SMILES in the dataset\n",
    "arbitrary_row = df.iloc[42]\n",
    "for benchmark_name, benchmark_func in benchmark_function_dict.items():\n",
    "    objective_val, _ = benchmark_func(arbitrary_row.smiles)\n",
    "    assert np.isclose(objective_val, arbitrary_row[f\"obj_{benchmark_name}\"], atol=1e-2, rtol=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run the methods and print results (the top molecules)\n",
    "\n",
    "We use a LRU cache to ensure that we correctly count the number of calls to the objective functions,\n",
    "and track the top molecules over time to provide insight into how performance changes over time.\n",
    "Note that the code for the actual algorithm and the result aggregation is in another file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some functions for printing results\n",
    "EVAL_TIMES = [3, 4, OBJECTIVE_EVAL_BUDGET]\n",
    "TOP_K = 3\n",
    "from de_novo_design_utils import generate_smiles, topk_mols_over_time, print_results_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for benchmark_name, benchmark_func in benchmark_function_dict.items():\n",
    "    print(f\"## Start benchmark {benchmark_name} ##\")\n",
    "    random.seed(0)\n",
    "    np.random.seed(0)\n",
    "    \n",
    "    # Initialize the dataset\n",
    "    dataset = {\n",
    "        row[\"smiles\"]: (\n",
    "            row[f\"obj_{benchmark_name}\"], \n",
    "            {fname: row[fname] for fname in benchmark_func.base_functions.keys()}\n",
    "        )\n",
    "        for _, row in df.iterrows()\n",
    "    }\n",
    "    \n",
    "    # Use LRU cache to continue until the evaluation budget is reached.\n",
    "    # LRU cache prevents us from accidentally doing additional function calls.\n",
    "    cached_benchmark_func = functools.lru_cache(maxsize=None)(benchmark_func)\n",
    "    smiles_to_results = dict()  # This is where we store the results\n",
    "    while cached_benchmark_func.cache_info().misses < OBJECTIVE_EVAL_BUDGET:\n",
    "        \n",
    "        # Step 1: choose a SMILES to evaluate\n",
    "        # TODO: Replace this part with your algorithm\n",
    "        chosen_smiles = generate_smiles(dataset)\n",
    "        \n",
    "        # Step 2: evaluate objective function, if it has not already been evaluated and is not in the dataset\n",
    "        if chosen_smiles not in smiles_to_results and chosen_smiles not in dataset:\n",
    "        \n",
    "            # Call the benchmark function\n",
    "            current_result = cached_benchmark_func(chosen_smiles)\n",
    "\n",
    "            # Store results of function, and also how many times\n",
    "            # the function was called when this result was obtained\n",
    "            smiles_to_results[chosen_smiles] = (\n",
    "                current_result, cached_benchmark_func.cache_info().misses\n",
    "            )\n",
    "            \n",
    "            # Update the dataset with the new result\n",
    "            # TODO: replace this with your own dataset update\n",
    "            dataset[chosen_smiles] = current_result\n",
    "            \n",
    "            del current_result\n",
    "        \n",
    "        # Cleanup after this iteration\n",
    "        del chosen_smiles\n",
    "    \n",
    "    # Print results for this benchmark\n",
    "    results_over_time = topk_mols_over_time(smiles_to_results, times=EVAL_TIMES, k=TOP_K)\n",
    "    print_results_over_time(results_over_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
